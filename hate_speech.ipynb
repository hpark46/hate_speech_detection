{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speech.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9Dij79-qD2O",
        "outputId": "29221f47-00ea-485e-cace-7d823b4f542d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the f1 score using SGD Classifier is: 0.9127459366980326\n",
            "the f1 score using Linear SVC is: 0.9316539335122858\n",
            "the f1 score using Decision Tree Classifier is: 0.9039640375970576\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import html\n",
        "from bs4 import BeautifulSoup\n",
        "import regex as re\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "\n",
        "def split_sets(dataset):\n",
        "    split_data = [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]     # 0: train 1: validation 2: test\n",
        "\n",
        "    split_list = [random.choice(split_data) for i in range(len(dataset))]\n",
        "\n",
        "    # print(split_list.count(0)/len(split_list))\n",
        "    # print(split_list.count(1)/len(split_list))\n",
        "\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "\n",
        "    index = 0\n",
        "    for num in split_list:\n",
        "        if num == 0:\n",
        "            train_set.append(dataset[index])\n",
        "\n",
        "        else:\n",
        "            test_set.append(dataset[index])\n",
        "        index = index + 1\n",
        "\n",
        "    return np.array(train_set), np.array(test_set)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filtering(input):\n",
        "    holder = re.sub(r'#', ' #', input)\n",
        "    holder = re.sub(r'@', ' @', holder)\n",
        "    holder = re.sub(r'(http|https)\\S+', '', holder)                                          # getting rid of url\n",
        "    holder = BeautifulSoup(html.unescape(holder), 'html.parser').text                       # html parse\n",
        "    holder = ''.join(filter(lambda x: x.isalnum() or x in [' ', '#', '@'], holder)).lower() # filtering\n",
        "    return holder\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def process_data():\n",
        "    hate_data = pd.read_csv(\"/content/drive/MyDrive/hate_speech.csv\",\n",
        "                            names = [\"Index\", \"Is_Hate\", \"Tweet\"])\n",
        "    hate_data.head()\n",
        "\n",
        "    hate_data = np.array(hate_data)\n",
        "\n",
        "    index = 0\n",
        "    for sentence in hate_data[:,2]:                 # filtering hate_dataset\n",
        "        hate_data[index,2] = filtering(sentence)\n",
        "        index = index + 1\n",
        "\n",
        "    train_set, test_set = split_sets(hate_data)\n",
        "\n",
        "    train_x = train_set[:, 2]\n",
        "    test_x = test_set[:, 2]\n",
        "\n",
        "    train_y = train_set[:, 1]\n",
        "    train_y = train_y.astype('int')\n",
        "\n",
        "    test_y = test_set[:, 1]\n",
        "    test_y = test_y.astype('int')\n",
        "\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        ('count', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('classify', SGDClassifier())\n",
        "    ])\n",
        "\n",
        "    pipe_two = Pipeline([\n",
        "        ('count', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('classify', LinearSVC())\n",
        "    ])\n",
        "\n",
        "    pipe_three = Pipeline([\n",
        "        ('count', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('decision_tree', DecisionTreeClassifier())\n",
        "    ])\n",
        "\n",
        "\n",
        "    # SGD classifier\n",
        "    model = pipe.fit(train_x, train_y)\n",
        "    predict_y = model.predict(test_x)\n",
        "    print('the f1 score using SGD Classifier is: ' + str(f1_score(test_y, predict_y)))\n",
        "\n",
        "    # Linear SVC\n",
        "    model_two = pipe_two.fit(train_x, train_y)\n",
        "    predict_y_two = model_two.predict(test_x)\n",
        "    print('the f1 score using Linear SVC is: ' + str(f1_score(test_y, predict_y_two)))\n",
        "\n",
        "    # Decision Tree Classifier\n",
        "    model_three = pipe_three.fit(train_x, train_y)\n",
        "    predict_y_three = model_three.predict(test_x)\n",
        "    print('the f1 score using Decision Tree Classifier is: ' + str(f1_score(test_y, predict_y_three)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    process_data()\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}